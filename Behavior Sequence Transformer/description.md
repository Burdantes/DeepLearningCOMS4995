We explore the \emph{behavior sequence transformer} architecture proposed by \cite{BST}. It differs from the traditional methods in that it incorporates information provided by the behavior sequence, or the order of actions, of the user. When ordered sequentially, the dependency of the actions can be captured by the self-attention mechanism. The goal is: let the behavior sequence of a user by $S=(v_1,\dots,v_n)$ where $v_i$ is the $i$-th item clicked, learn the probability of clicking a target item $v_t$.

The BST builds on top of a Embedding\&MLP paradigm, namely, the architecture consists of three components bottum-up: an embedding layer, a transformer layer, and a MLP. For the embedding layer, the inputs are features of the user and features of the items. In particular, the item features are categorized into two types: Sequence Item Features and Positional Features, and this extra step helps record the order of user actions. The low-dimensional item features are then fed to a transformer layer, and the output of this transformer layer and the low-dimensional user features are used as input to train a classifier for the binary classification problem: whether the user will click $v_t$. The loss function used is the cross-entropy loss.
